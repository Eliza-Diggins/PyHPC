#----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|---#
#                     _/_/_/_/_/_/                 _/          _/   _/_/_/_/_/_/       _/_/_/_/_/                      #
#                     _/         _/                _/          _/   _/         _/    _/                                #
#                     _/         _/  _/      _/    _/          _/   _/         _/   _/                                 #
#                     _/_/_/_/_/_/   _/      _/    _/_/_/_/_/_/_/   _/_/_/_/_/_/    _/                                 #
#                     _/              _/_/_/_/     _/          _/   _/              _/                                 #
#                     _/                  _/       _/          _/   _/               _/                                #
#                     _/             _/_/_/        _/          _/   _/                 _/_/_/_/_/                      #
#                   #-----------------------------------------------------------------------------#                    #
#                    SLURM Scheduler Settings                                                                          #
#----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|---#
[META] # This region of the settings can be used to write custom scripts without interupting normal functionality.
updated = {v="5/15/23",d="5/15/23",i=""}
[Settings]
t = {v="24:00:00",d="24:00:00",i="The runtime for the SLURM script."}
N = {v=16,d=16,i="The default number of nodes"}
n = {v=214,d=214,i="The number of MPI tasks (number of cores)"}
A = {v="wik",d="wik",i="The account information for CHPC"}
p = {v="notchpeak",d="notchpeak",i="The cluster to use for the computation."}

[files]
format = {v="slurm_%(name)s_%(date)s",d="slurm_%(name)s_%(date)s",i="The .err and .out file paths"}